{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project: Fraud Detection for STEG\n",
    "\n",
    "The Tunisian Company of Electricity and Gas (STEG) is a public and a non-administrative company, it is responsible for delivering electricity and gas across Tunisia. The company suffered tremendous losses in the order of 200 million Tunisian Dinars due to fraudulent manipulations of meters by consumers. Using the client’s billing history, our aim is to detect and recognize clients involved in fraudulent activities.\n",
    "\n",
    "## Data Product\n",
    "\n",
    "- Goal: develop fraud-detection system that identifies clients involved in fraudulent activities while leaving non-fraudulent clients aside. \n",
    "- Value of Product: enhance the company’s revenues by reducing the losses caused by fraudulent activities, avoid reputation damage.\n",
    "- Evaluation Metric: ROC-AUC (True Positive Rate and True Negative Rate will also be inspected).\n",
    "- Baseline Model: coin toss, since no intuitive baseline model gave good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, accuracy_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve\n",
    "import pickle\n",
    "from xgboost import cv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(context='notebook', style='darkgrid')\n",
    "\n",
    "RSEED = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning\n",
    "\n",
    "The following column documentation was provided by the STEG. Unfortunately, this is not a very good documentation. Some columns are left unexplained, and most explanations are not helpful.\n",
    "\n",
    "- Client_id: Unique id for client\n",
    "- District: District where the client is\n",
    "- Client_catg: Category client belongs to\n",
    "- Region: Area where the client is\n",
    "- Creation_date: Date client joined\n",
    "- Target: fraud:1 , not fraud: 0\n",
    "- Invoice_date: Date of the invoice\n",
    "- Tarif_type: Type of tax\n",
    "- Counter_number:\n",
    "- Counter_statue: takes up to 5 values such as working fine, not working, on hold statue, ect\n",
    "- Counter_code:\n",
    "- Reading_remarque: notes that the STEG agent takes during his visit to the client (e.g: If the counter shows something wrong, the agent gives a bad score)\n",
    "- Counter_coefficient: An additional coefficient to be added when standard consumption is exceeded\n",
    "- Consommation_level_1: Consumption_level_1\n",
    "- Consommation_level_2: Consumption_level_2\n",
    "- Consommation_level_3: Consumption_level_3\n",
    "- Consommation_level_4: Consumption_level_4\n",
    "- Old_index: Old index\n",
    "- New_index: New index\n",
    "- Months_number: Month number\n",
    "- Counter_type: Type of counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped duplicate rows: 11\n",
      "Number of nulls: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4_476_691, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>client_id</th><th>invoice_date</th><th>tarif_type</th><th>counter_number</th><th>counter_statue</th><th>counter_code</th><th>reading_remarque</th><th>counter_coefficient</th><th>consommation_level_1</th><th>consommation_level_2</th><th>consommation_level_3</th><th>consommation_level_4</th><th>old_index</th><th>new_index</th><th>months_number</th><th>counter_type</th><th>disrict</th><th>client_catg</th><th>region</th><th>creation_date</th><th>target</th></tr><tr><td>str</td><td>datetime[μs]</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>datetime[μs]</td><td>i8</td></tr></thead><tbody><tr><td>&quot;train_Client_23754&quot;</td><td>2017-06-09 00:00:00</td><td>40</td><td>6780809</td><td>0</td><td>5</td><td>6</td><td>1</td><td>122</td><td>0</td><td>0</td><td>0</td><td>3166</td><td>3288</td><td>4</td><td>0</td><td>63</td><td>11</td><td>311</td><td>1997-05-27 00:00:00</td><td>1</td></tr><tr><td>&quot;train_Client_106992&quot;</td><td>2005-11-25 00:00:00</td><td>11</td><td>51741</td><td>0</td><td>203</td><td>6</td><td>1</td><td>314</td><td>0</td><td>0</td><td>0</td><td>9033</td><td>9347</td><td>4</td><td>1</td><td>63</td><td>11</td><td>312</td><td>1998-12-31 00:00:00</td><td>0</td></tr><tr><td>&quot;train_Client_110329&quot;</td><td>2011-07-09 00:00:00</td><td>15</td><td>8618662</td><td>0</td><td>202</td><td>6</td><td>1</td><td>518</td><td>0</td><td>0</td><td>0</td><td>46660</td><td>47178</td><td>4</td><td>1</td><td>69</td><td>11</td><td>103</td><td>1979-02-20 00:00:00</td><td>1</td></tr><tr><td>&quot;train_Client_129249&quot;</td><td>2008-08-22 00:00:00</td><td>11</td><td>199083</td><td>0</td><td>203</td><td>6</td><td>1</td><td>329</td><td>0</td><td>0</td><td>0</td><td>7694</td><td>8023</td><td>4</td><td>1</td><td>60</td><td>11</td><td>101</td><td>2007-06-07 00:00:00</td><td>0</td></tr><tr><td>&quot;train_Client_16844&quot;</td><td>2008-06-27 00:00:00</td><td>11</td><td>849881</td><td>0</td><td>203</td><td>6</td><td>1</td><td>266</td><td>0</td><td>0</td><td>0</td><td>4701</td><td>4967</td><td>4</td><td>1</td><td>63</td><td>11</td><td>311</td><td>2003-08-28 00:00:00</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;train_Client_7117&quot;</td><td>2007-05-06 00:00:00</td><td>40</td><td>6777548</td><td>0</td><td>5</td><td>6</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>341</td><td>341</td><td>2</td><td>0</td><td>62</td><td>11</td><td>301</td><td>2004-12-23 00:00:00</td><td>0</td></tr><tr><td>&quot;train_Client_42270&quot;</td><td>2012-07-03 00:00:00</td><td>11</td><td>105049</td><td>0</td><td>207</td><td>9</td><td>1</td><td>648</td><td>0</td><td>0</td><td>0</td><td>3483</td><td>4131</td><td>4</td><td>1</td><td>62</td><td>11</td><td>301</td><td>2010-05-20 00:00:00</td><td>0</td></tr><tr><td>&quot;train_Client_64157&quot;</td><td>2017-02-06 00:00:00</td><td>11</td><td>1301</td><td>0</td><td>532</td><td>9</td><td>1</td><td>200</td><td>100</td><td>200</td><td>2267</td><td>335290</td><td>338057</td><td>1</td><td>1</td><td>63</td><td>51</td><td>312</td><td>2008-01-21 00:00:00</td><td>0</td></tr><tr><td>&quot;train_Client_107868&quot;</td><td>2008-12-02 00:00:00</td><td>11</td><td>162281</td><td>0</td><td>413</td><td>6</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>25</td><td>26</td><td>4</td><td>1</td><td>69</td><td>11</td><td>107</td><td>2007-08-13 00:00:00</td><td>0</td></tr><tr><td>&quot;train_Client_14090&quot;</td><td>2008-10-21 00:00:00</td><td>11</td><td>740341</td><td>0</td><td>203</td><td>6</td><td>1</td><td>382</td><td>0</td><td>0</td><td>0</td><td>6615</td><td>6997</td><td>4</td><td>1</td><td>69</td><td>11</td><td>104</td><td>2003-02-27 00:00:00</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4_476_691, 21)\n",
       "┌────────────┬────────────┬────────────┬────────────┬───┬────────────┬────────┬───────────┬────────┐\n",
       "│ client_id  ┆ invoice_da ┆ tarif_type ┆ counter_nu ┆ … ┆ client_cat ┆ region ┆ creation_ ┆ target │\n",
       "│ ---        ┆ te         ┆ ---        ┆ mber       ┆   ┆ g          ┆ ---    ┆ date      ┆ ---    │\n",
       "│ str        ┆ ---        ┆ i64        ┆ ---        ┆   ┆ ---        ┆ i64    ┆ ---       ┆ i8     │\n",
       "│            ┆ datetime[μ ┆            ┆ i64        ┆   ┆ i64        ┆        ┆ datetime[ ┆        │\n",
       "│            ┆ s]         ┆            ┆            ┆   ┆            ┆        ┆ μs]       ┆        │\n",
       "╞════════════╪════════════╪════════════╪════════════╪═══╪════════════╪════════╪═══════════╪════════╡\n",
       "│ train_Clie ┆ 2017-06-09 ┆ 40         ┆ 6780809    ┆ … ┆ 11         ┆ 311    ┆ 1997-05-2 ┆ 1      │\n",
       "│ nt_23754   ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 7         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2005-11-25 ┆ 11         ┆ 51741      ┆ … ┆ 11         ┆ 312    ┆ 1998-12-3 ┆ 0      │\n",
       "│ nt_106992  ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 1         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2011-07-09 ┆ 15         ┆ 8618662    ┆ … ┆ 11         ┆ 103    ┆ 1979-02-2 ┆ 1      │\n",
       "│ nt_110329  ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 0         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2008-08-22 ┆ 11         ┆ 199083     ┆ … ┆ 11         ┆ 101    ┆ 2007-06-0 ┆ 0      │\n",
       "│ nt_129249  ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 7         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2008-06-27 ┆ 11         ┆ 849881     ┆ … ┆ 11         ┆ 311    ┆ 2003-08-2 ┆ 0      │\n",
       "│ nt_16844   ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 8         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ …          ┆ …          ┆ …          ┆ …          ┆ … ┆ …          ┆ …      ┆ …         ┆ …      │\n",
       "│ train_Clie ┆ 2007-05-06 ┆ 40         ┆ 6777548    ┆ … ┆ 11         ┆ 301    ┆ 2004-12-2 ┆ 0      │\n",
       "│ nt_7117    ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 3         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2012-07-03 ┆ 11         ┆ 105049     ┆ … ┆ 11         ┆ 301    ┆ 2010-05-2 ┆ 0      │\n",
       "│ nt_42270   ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 0         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2017-02-06 ┆ 11         ┆ 1301       ┆ … ┆ 51         ┆ 312    ┆ 2008-01-2 ┆ 0      │\n",
       "│ nt_64157   ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 1         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2008-12-02 ┆ 11         ┆ 162281     ┆ … ┆ 11         ┆ 107    ┆ 2007-08-1 ┆ 0      │\n",
       "│ nt_107868  ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 3         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "│ train_Clie ┆ 2008-10-21 ┆ 11         ┆ 740341     ┆ … ┆ 11         ┆ 104    ┆ 2003-02-2 ┆ 0      │\n",
       "│ nt_14090   ┆ 00:00:00   ┆            ┆            ┆   ┆            ┆        ┆ 7         ┆        │\n",
       "│            ┆            ┆            ┆            ┆   ┆            ┆        ┆ 00:00:00  ┆        │\n",
       "└────────────┴────────────┴────────────┴────────────┴───┴────────────┴────────┴───────────┴────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_client = pl.read_csv(\"data/client_train.csv\", ignore_errors=True)\n",
    "df_invoice = pl.read_csv(\"data/invoice_train.csv\", ignore_errors=True)\n",
    "\n",
    "# Merging\n",
    "df = df_invoice.join(df_client, on = \"client_id\")\n",
    "\n",
    "# Dropping duplicates\n",
    "duplicates = len(df) - len(df.unique())\n",
    "print(f\"Number of dropped duplicate rows: {duplicates}\")\n",
    "df = df.unique()\n",
    "\n",
    "# Converting counter_type from string to integer\n",
    "df = df.with_columns(pl.col([\"counter_type\"]).replace_strict({\"ELEC\": 1, \"GAZ\": 0}))\n",
    "\n",
    "# Cleaning counter_statue column\n",
    "df = df.filter(pl.col([\"counter_statue\"]) <=5)\n",
    "\n",
    "# Replace Nan with Null values\n",
    "df.with_columns(pl.col(pl.Float32, pl.Float64).fill_nan(None))\n",
    "\n",
    "# Null values\n",
    "nulls = int(df.null_count().sum_horizontal().to_numpy())\n",
    "print(f\"Number of nulls: {nulls}\")\n",
    "\n",
    "# Ruling out transactions earlier than 2005\n",
    "df = df.with_columns(pl.col([\"invoice_date\"]).str.to_datetime(format=\"%Y-%m-%d\"))\n",
    "df = df.with_columns(pl.col([\"creation_date\"]).str.to_datetime(format=\"%d/%m/%Y\"))\n",
    "\n",
    "# Chaning data type of target\n",
    "df = df.with_columns(df[\"target\"].cast(pl.Int8))\n",
    "\n",
    "# Overview of the data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical overview\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electricity Fraud vs. Gas Fraud\n",
    "\n",
    "There is *much* more fraudulent activity in electricity than in gas. And obviously, there in much more non-fraudulent than fraudulent activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x = \"counter_type\", hue = \"target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Since our dataset is highly unbalanced, we *undersample* the majority class, i.e. non-fraudulent cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing features to convert\n",
    "# nominal_variables = [\"tarif_type\",\"counter_statue\",\"reading_remarque\",\"disrict\",\"client_catg\",\"region\"]\n",
    "#df=df.to_dummies(nominal_variables)\n",
    "\n",
    "# Dropping features\n",
    "dropped_features = [\"client_id\",\"counter_number\",\"counter_code\",\"invoice_date\",\"target\"]\n",
    "\n",
    "# Defining target and features\n",
    "y = np.array(df[\"target\"])\n",
    "X = np.array(df.drop(dropped_features))\n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=RSEED)\n",
    "\n",
    "# Assuming X_train, y_train are your feature and target datasets\n",
    "# Perform downsampling on the majority class\n",
    "X_downsampled, y_downsampled = resample(X_train[y_train == 0],\n",
    "                                         y_train[y_train == 0],\n",
    "                                         replace=False,\n",
    "                                         n_samples=np.sum(y_train == 1),\n",
    "                                         random_state=RSEED)\n",
    "\n",
    "# Concatenate the downsampled data with the minority class data\n",
    "X_balanced = np.concatenate((X_downsampled, X_train[y_train == 1]))\n",
    "y_balanced = np.concatenate((y_downsampled, y_train[y_train == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Our EDA gave no interesting results as to which feature could serve as a good baseline predictor. We therefore take a simple coinflip as baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "t = time.time()\n",
    "\n",
    "# Creating column with coin flip results\n",
    "number_rows = len(y_test)\n",
    "y_test_pred_coin = np.random.randint(2, size=number_rows)\n",
    "\n",
    "# Stop timer\n",
    "elapsed = time.time() - t\n",
    "\n",
    "# Plotting \n",
    "cf_matrix = confusion_matrix(y_test,y_test_pred_coin)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix for Coin Flip\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_coin)\n",
    "tpr = recall_score(y_test, y_test_pred_coin)\n",
    "tnr = recall_score(y_test, y_test_pred_coin, pos_label=0)\n",
    "\n",
    "# Store metrics\n",
    "coin_metrics = pl.DataFrame({\n",
    "    \"Model\": \"Coin Flip\", \n",
    "    \"ROC-AUC\": [roc_auc], \n",
    "    \"True Positive Rate\": [tpr], \n",
    "    \"True Negative Rate\": [tnr],\n",
    "    \"Elapsed Time in Seconds\": [elapsed],\n",
    "})\n",
    "\n",
    "# \n",
    "display(coin_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Model</th><th>ROC-AUC</th><th>True Positive Rate</th><th>True Negative Rate</th><th>Elapsed Time in Seconds</th></tr></thead><tbody><tr><td>&quot;DecisionTree&quot;</td><td>0.807553</td><td>0.817281</td><td>0.796661</td><td>7.162922</td></tr><tr><td>&quot;RandomForest&quot;</td><td>0.850744</td><td>0.773744</td><td>0.759178</td><td>58.744354</td></tr><tr><td>&quot;LightGBM&quot;</td><td>0.723921</td><td>0.676871</td><td>0.644608</td><td>5.042496</td></tr><tr><td>&quot;XGBoost&quot;</td><td>0.760436</td><td>0.692579</td><td>0.681662</td><td>3.225021</td></tr><tr><td>&quot;CatBoost&quot;</td><td>0.777287</td><td>0.710232</td><td>0.69272</td><td>53.399082</td></tr><tr><td>&quot;Naive Bayes&quot;</td><td>0.597626</td><td>0.994553</td><td>0.008388</td><td>1.363348</td></tr><tr><td>&quot;Logistic Regression&quot;</td><td>0.623053</td><td>0.596075</td><td>0.585133</td><td>2.83381</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 5)\n",
       "┌─────────────────────┬──────────┬────────────────────┬────────────────────┬─────────────────┐\n",
       "│ Model               ┆ ROC-AUC  ┆ True Positive Rate ┆ True Negative Rate ┆ Elapsed Time in │\n",
       "│                     ┆          ┆                    ┆                    ┆ Seconds         │\n",
       "╞═════════════════════╪══════════╪════════════════════╪════════════════════╪═════════════════╡\n",
       "│ DecisionTree        ┆ 0.807553 ┆ 0.817281           ┆ 0.796661           ┆ 7.162922        │\n",
       "│ RandomForest        ┆ 0.850744 ┆ 0.773744           ┆ 0.759178           ┆ 58.744354       │\n",
       "│ LightGBM            ┆ 0.723921 ┆ 0.676871           ┆ 0.644608           ┆ 5.042496        │\n",
       "│ XGBoost             ┆ 0.760436 ┆ 0.692579           ┆ 0.681662           ┆ 3.225021        │\n",
       "│ CatBoost            ┆ 0.777287 ┆ 0.710232           ┆ 0.69272            ┆ 53.399082       │\n",
       "│ Naive Bayes         ┆ 0.597626 ┆ 0.994553           ┆ 0.008388           ┆ 1.363348        │\n",
       "│ Logistic Regression ┆ 0.623053 ┆ 0.596075           ┆ 0.585133           ┆ 2.83381         │\n",
       "└─────────────────────┴──────────┴────────────────────┴────────────────────┴─────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model list\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"RandomForest\": RandomForestClassifier(n_jobs=-1),\n",
    "    #\"ExtraTrees\": ExtraTreesClassifier(n_jobs=-1),\n",
    "    #\"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier(n_jobs=-1,verbosity=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_jobs=-1),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0,),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Initiate storage for each model\n",
    "all_metrics = pl.DataFrame()\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    # Start timer\n",
    "    t = time.time()\n",
    "\n",
    "    # Initializing and fitting pipeline with standard scaler\n",
    "    pipe = Pipeline([('scale', StandardScaler()),(model_name, model)]).fit(X_balanced, y_balanced)\n",
    "    \n",
    "    # Predict\n",
    "    y_test_pred = pipe.predict(X_test)\n",
    "    y_test_pred_prob = pipe.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Stop timer\n",
    "    elapsed = time.time() - t\n",
    "    \n",
    "    # # Confusion Matrix\n",
    "    # cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    # # Plotting Confusion Matrix\n",
    "    # sns.heatmap(cf_matrix / cf_matrix.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap='Blues')\n",
    "    # plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    # plt.xlabel(\"Predicted label\")\n",
    "    # plt.ylabel(\"True label\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred_prob) if y_test_pred_prob is not None else None\n",
    "    tpr = recall_score(y_test, y_test_pred)\n",
    "    tnr = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "    \n",
    "    # Store metrics\n",
    "    model_metrics = pl.DataFrame({\n",
    "        \"Model\": [model_name], \n",
    "        \"ROC-AUC\": [roc_auc], \n",
    "        \"True Positive Rate\": [tpr], \n",
    "        \"True Negative Rate\": [tnr],\n",
    "        \"Elapsed Time in Seconds\": [elapsed],\n",
    "    })\n",
    "    \n",
    "    # Concatenate\n",
    "    all_metrics = pl.concat([all_metrics, model_metrics])\n",
    "\n",
    "# Display metrics\n",
    "pl.Config.set_tbl_hide_column_data_types(True)  \n",
    "display(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameter grid (as dictionary)\n",
    "param_grid = {\"n_estimators\": [1500],\n",
    "            \"max_depth\": [10],\n",
    "            'min_child_weight': [1],\n",
    "            \"learning_rate\": [.3],\n",
    "            \"n_jobs\": [-1],\n",
    "              }\n",
    "\n",
    "# Instantiate search and define the metric to optimize \n",
    "gs = RandomizedSearchCV(XGBClassifier(), param_grid, scoring=\"roc_auc\", cv=2, verbose=5, n_jobs=1).fit(X_balanced, y_balanced)\n",
    "\n",
    "# Predict\n",
    "y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "y_test_pred_prob = gs.best_estimator_.predict_proba(X_test)[:, 1] if hasattr(gs.best_estimator_, \"predict_proba\") else None\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "sns.heatmap(cf_matrix / cf_matrix.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix for Optimized XGBoost\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_prob) if y_test_pred_prob is not None else None\n",
    "tpr = recall_score(y_test, y_test_pred)\n",
    "tnr = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "# Store metrics\n",
    "model_metrics = pl.DataFrame({\n",
    "    \"Model\": \"Optimized XGBoost\", \n",
    "    \"ROC-AUC\": roc_auc, \n",
    "    \"True Positive Rate\": tpr, \n",
    "    \"True Negative Rate\": tnr\n",
    "})\n",
    "\n",
    "# Present results\n",
    "pl.Config.set_tbl_hide_column_data_types(True)  \n",
    "display(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720464</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.707089</td>\n",
       "      <td>0.001087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.739855</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.723134</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.757512</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.737013</td>\n",
       "      <td>0.002807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.769866</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.745563</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780871</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.753929</td>\n",
       "      <td>0.001121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0        0.720464       0.000691       0.707089      0.001087\n",
       "1        0.739855       0.001025       0.723134      0.000819\n",
       "2        0.757512       0.001360       0.737013      0.002807\n",
       "3        0.769866       0.001465       0.745563      0.000440\n",
       "4        0.780871       0.001563       0.753929      0.001121"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate search and define the metric to optimize \n",
    "gs = RandomizedSearchCV(XGBClassifier(), param_grid, scoring=\"roc_auc\", cv=2, verbose=5, n_jobs=1).fit(X_balanced, y_balanced)\n",
    "\n",
    "# Predict\n",
    "y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "y_test_pred_prob = gs.best_estimator_.predict_proba(X_test)[:, 1] if hasattr(gs.best_estimator_, \"predict_proba\") else None\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "sns.heatmap(cf_matrix / cf_matrix.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix for Optimized XGBoost\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_prob) if y_test_pred_prob is not None else None\n",
    "tpr = recall_score(y_test, y_test_pred)\n",
    "tnr = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "# Store metrics\n",
    "model_metrics = pl.DataFrame({\n",
    "    \"Model\": \"Optimized XGBoost\", \n",
    "    \"ROC-AUC\": roc_auc, \n",
    "    \"True Positive Rate\": tpr, \n",
    "    \"True Negative Rate\": tnr\n",
    "})\n",
    "\n",
    "# Present results\n",
    "pl.Config.set_tbl_hide_column_data_types(True)  \n",
    "display(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict\n",
    "y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "y_test_pred_prob = gs.best_estimator_.predict_proba(X_test)[:, 1] if hasattr(gs.best_estimator_, \"predict_proba\") else None\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "sns.heatmap(cf_matrix / cf_matrix.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix for Optimized XGBoost\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_prob) if y_test_pred_prob is not None else None\n",
    "tpr = recall_score(y_test, y_test_pred)\n",
    "tnr = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "# Store metrics\n",
    "model_metrics = pl.DataFrame({\n",
    "    \"Model\": \"Optimized XGBoost\", \n",
    "    \"ROC-AUC\": roc_auc, \n",
    "    \"True Positive Rate\": tpr, \n",
    "    \"True Negative Rate\": tnr\n",
    "})\n",
    "\n",
    "# Present results\n",
    "pl.Config.set_tbl_hide_column_data_types(True)  \n",
    "display(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation and Hyperparameter Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining ranges of hyperparameter for random search\n",
    "estimator_num = np.arange(1,30,2)**2\n",
    "depth = [None] + list(range(1,50,5))\n",
    "var_range = np.arange(1,20,3)\n",
    "criterion = [\"gini\", \"log_loss\",\"entropy\"]\n",
    "learning_rate = np.linspace(0.01, 1, 6)\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    \"DecisionTree\": {\n",
    "        \"max_depth\": depth,\n",
    "        \"min_samples_split\": var_range,\n",
    "        \"min_samples_leaf\": var_range,\n",
    "        \"criterion\": criterion,\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": estimator_num,\n",
    "        \"max_depth\": depth,\n",
    "        \"min_samples_split\": var_range,\n",
    "        \"min_samples_leaf\": var_range,\n",
    "        \"n_jobs\": [-1],\n",
    "    },\n",
    "    \"ExtraTrees\": {\n",
    "        \"n_estimators\": estimator_num,\n",
    "        \"max_depth\": depth,\n",
    "        \"min_samples_split\": var_range,\n",
    "        \"min_samples_leaf\": var_range,\n",
    "        \"criterion\": criterion,\n",
    "        \"n_jobs\": [-1],\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": estimator_num,\n",
    "        \"max_depth\": depth,\n",
    "        \"min_child_weight\": var_range,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_jobs\": [-1],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    #\"DecisionTree\": DecisionTreeClassifier(),\n",
    "    #\"RandomForest\": RandomForestClassifier(),\n",
    "    #\"ExtraTrees\": ExtraTreesClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "}\n",
    "\n",
    "# Empty list to store model metrics\n",
    "model_metrics_list = []\n",
    "hyperparameters_list = []\n",
    "\n",
    "# Loop over models and perform RandomizedSearchCV\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Instantiate RandomizedSearchCV\n",
    "    rs = GridSearchCV(model, param_grids[model_name], scoring=\"roc_auc\", cv=5, verbose=0, n_jobs=1).fit(X_balanced, y_balanced)\n",
    "    \n",
    "    # Pickle model\n",
    "    pickle.dump(rs.best_estimator_, open(\"models/\"+model_name, 'wb'))\n",
    "\n",
    "    # Predict on test data\n",
    "    y_test_pred = rs.best_estimator_.predict(X_test)\n",
    "    y_test_pred_prob = rs.best_estimator_.predict_proba(X_test)[:, 1] if hasattr(rs.best_estimator_, \"predict_proba\") else None\n",
    "    \n",
    "    # # Confusion Matrix\n",
    "    # cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    # # Plot Confusion Matrix\n",
    "    # sns.heatmap(cf_matrix / cf_matrix.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap='Blues')\n",
    "    # plt.title(f\"Confusion Matrix for Optimized {model_name}\")\n",
    "    # plt.xlabel(\"Predicted label\")\n",
    "    # plt.ylabel(\"True label\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred_prob) if y_test_pred_prob is not None else None\n",
    "    tpr = recall_score(y_test, y_test_pred)\n",
    "    tnr = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"True Positive Rate\": tpr,\n",
    "        \"True Negative Rate\": tnr,\n",
    "    }\n",
    "\n",
    "    # Append hyperparameters to model metrics\n",
    "    metrics.update(rs.best_params_)\n",
    "\n",
    "    # Store model metrics\n",
    "    model_metrics_list.append(metrics)\n",
    "\n",
    "\n",
    "# Convert list of metrics and hyperparameters to DataFrame\n",
    "model_metrics = pl.DataFrame(model_metrics_list)\n",
    "\n",
    "# Present results\n",
    "pl.Config.set_tbl_hide_column_data_types(True)\n",
    "display(model_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC/ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test,  y_test_pred_prob)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_title('ROC Curve for Tuned Decision Tree')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importance plus labels\n",
    "feat_importance = gs.best_estimator_.feature_importances_\n",
    "x_labels = df.drop(dropped_features).columns\n",
    "\n",
    "pl.DataFrame({\"Feature\": x_labels, \"Importance\": feat_importance}).sort(by=\"Importance\",descending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1028</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,504</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1028</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1028</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,057,812</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1028</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,029</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)             │            \u001b[38;5;34m68\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1028\u001b[0m)           │        \u001b[38;5;34m18,504\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1028\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1028\u001b[0m)           │     \u001b[38;5;34m1,057,812\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1028\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,029\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,413</span> (4.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,077,413\u001b[0m (4.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,379</span> (4.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,077,379\u001b[0m (4.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> (136.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34\u001b[0m (136.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3090/3090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 13ms/step - AUC: 0.6422 - loss: 0.6305 - val_AUC: 0.0000e+00 - val_loss: 0.8899\n",
      "Epoch 2/100\n",
      "\u001b[1m3090/3090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - AUC: 0.6606 - loss: 0.6210 - val_AUC: 0.0000e+00 - val_loss: 0.9182\n",
      "Epoch 3/100\n",
      "\u001b[1m3090/3090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - AUC: 0.6613 - loss: 0.6204 - val_AUC: 0.0000e+00 - val_loss: 0.9517\n",
      "Epoch 4/100\n",
      "\u001b[1m3090/3090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - AUC: 0.6676 - loss: 0.6172 - val_AUC: 0.0000e+00 - val_loss: 0.9673\n",
      "Epoch 5/100\n",
      "\u001b[1m3090/3090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12ms/step - AUC: 0.6675 - loss: 0.6175 - val_AUC: 0.0000e+00 - val_loss: 0.8462\n",
      "Epoch 6/100\n",
      "\u001b[1m3090/3090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - AUC: 0.6710 - loss: 0.6148 - val_AUC: 0.0000e+00 - val_loss: 0.9521\n",
      "Epoch 7/100\n",
      "\u001b[1m1403/3090\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - AUC: 0.6755 - loss: 0.6135"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#y_test_pred = model.predict(X_test)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#roc_auc_score(y_test, y_test_pred)\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/neuefische/ml_project_neuefische/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Number of features (input dimension)\n",
    "input_shape = [X_balanced.shape[1]]\n",
    "\n",
    "# Define callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.01,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer with 17 features and a hidden layer with 32 neurons\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "model.add(Dense(1028,activation='relu')),\n",
    "#model.add(BatchNormalization(input_shape=input_shape))\n",
    "model.add(Dropout(rate=.3))\n",
    "model.add(Dense(1028, activation='relu'))\n",
    "#model.add(BatchNormalization(input_shape=input_shape)),\n",
    "model.add(Dropout(rate=.3))\n",
    "\n",
    "# Output layer with 1 neuron and sigmoid activation for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                loss=BinaryCrossentropy(),\n",
    "                metrics=['AUC'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_balanced, y_balanced, epochs=100, batch_size=128, validation_split=0.2,callbacks=[early_stopping])\n",
    "\n",
    "# Predict\n",
    "#y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "#roc_auc_score(y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
